<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[伊 始]]></title>
    <url>%2F2018%2F07%2F11%2Fbeginning%2F</url>
    <content type="text"><![CDATA[开始的开始最近几天的西安城，阴雨绵绵，到哪都是湿漉漉的一片，给人一种身处烟雨江南的感觉。走在人少路窄的街道上，你也许能体会到戴望舒那“独自徘徊在悠长、悠长又寂寥的雨巷”朦胧而又幽深的美感，还会期待着“逢着一个丁香一样的结着愁怨的姑娘”。说实话，这不是西安第一次让我产生这样的错觉，今年夏天的西安，似乎更受雨水的青睐，总在温度快速攀升指至30多度的时候迎来一场及时雨，之后又快速回落，这便被网友玩成了“满30立减20”的梗。的确，今年夏天少了一些往年的燥热，多了几分夏末秋初的凉爽。 善“思”我喜欢下雨天，不仅仅是因为雨天凉快，更重要的是我喜欢雨后万物如新洗的明丽和纯粹，洗去一身尘垢，显得本真色彩，这种雨后独有的清新秀丽在其他时间是看不到、体会不到的。其实，还有一个重要原因是我喜欢时不时地“胡思乱想”。一直觉得雨天最适合“天马行空”，尤其是雨后，觅一幽静之处，檐下雨水滴答，袖间凉风习习，雾气散尽之后，溢成满屋花香。这个时候，放空脑袋，忘却平日的紧张与忙碌，心随着眼睛漫无目的的在近处、远处、高出、低处随意游荡。当有趣的事物映入眼帘，定睛多看几眼，之后便会心一笑。仰望天空，乌云未散，依旧能感受到“黑云压城城欲摧”的气势。凝望着天空，就好似凝望着无尽的深渊，而此时，深渊也在凝望着你，看得愈久，便陷得愈深，这便开始了天马行空般的“胡思乱想”。可能会从日常闲事想到人生哲理，也许会从方隅之间想到世界宇宙，会想东想西，会“思前想后”。这种思考不一定有多大意义，甚至可能有人认为毫无意义，但最重要的是这可以放松身心，在匆忙疲惫的生活中在精神上宽慰自己。心里的豁然开朗、风轻云淡，还有思维的释放，用语言很那表达这种奇妙的感觉。据说每天给自己一定时间的冥想，能很好地平静内心，缓解压力，促进身心健康，更重要的是，也许某一时刻的灵光闪动，就会是改变你一生的“箴言”。 最近一直阴雨天，加上近来工作也相对清闲，便有了较多的时间做自己想做的事情，这对于我，可以说是如获至宝。因为真正忙碌的时候，没有时间更重要是没有心思去做其他的事情，也可能是长时间的“养老”氛围让我有些安逸和懒惰了。这一段时间，一直觉得过得有些浑浑噩噩，过于单调乏味。白天上班，空余看看技术文章，找找技术盲点研究研究，时间长了就会有些许疲惫。下班之后，便起身回到住处（我不太喜欢把租的公寓称之为“家”，不论是合租还是自己一个人住，因为在我觉得那不是“家”，称之为“住处”更为合适），回去便是“咸鱼躺”，然后刷刷新闻、微博，看看综艺（《向往的生活》一直是我很喜欢的一档综艺，很慢生活，很田园，推荐可以看看），隔三差五洗洗衣服。而且，几乎每天都是这样，这让我每天起床睁眼，总觉得被“困”在了同一天，循环这一天的所有，就像电影《土拨鼠之日》一样。 这样的生活也有一段时了，刚开始没感到有什么不妥，也没察觉其中的可怕之处。这几天，总是觉得被这一天“困”得无法前进，觉得自己更像是一个“智能机器”，醒来之后遍重复着几乎相同的事情，睡着后什么都像是忘掉了，可能连“梦”都没有。都说不做梦，是因为睡眠质量好，可一直不做梦是不是也有些可怜甚至可悲呢？这种循环最可怕的在于，你的记忆里便只有了早上起床洗漱、挤公交上班、枯燥地工作8小时甚至十几个小时、挤公交下班、洗漱睡觉这些事情，然后一天天地反复循环，没有尽头……这样的生活还会给人一种可怕的感受 —— “时间过得真快”，对于一般的工薪上班族，这种感觉应该是格外明显。在我看来，这种生活是可怜的，也是可怕的，时间久了，如果渐渐适应或者说妥协，是一件很可怕的事情，就像“青蛙效应”。毕竟从“智人”进化而来的人类，是“智”、“思考”造就了今天的我们，生而为人，可如果每天扮演一个“智能机器”的角色，还真是很不甘心啊…… 善“思”，亦应善“行”与其被困在原地，每天浑浑噩噩，那为什么不做一些改变呢？如果说没有意识到，或者说习惯了循环式的生活，那就另当别论。但如果选择改变，选择不妥协，那就应该做出改变。工作学习之余，合理安排时间，看看书，练练字，写写博客，跑跑步，健健身，学学厨艺，学学摄影，随处走走停停……对了，每天给自己冥想的时间，也是一个不错的选择，在善“思”的同时，亦应该做到善“行”。 每个人都应该有自己的爱好，有自己的乐趣所在，也应该有自己想要做但没来得及做的事情。一切都不算晚，一切都还有机会，最重要的是学会开始，毕竟“万事开头难”。改变并不意味着要短时间的巨变，给自己一个过渡的时间，一个循序渐进的过程，“量变到质变”是一个永恒不变的真理。 所以，我想，首先学会写博客，把自己的生活记录下来，再分享给大家。不一定要有很多人看，更不奢求有什么粉丝，只是希望看到文章的人在读到某一行，某一句的时候，会心一笑或者能帮你拨去心的乌云。其实最简单的目的其实还是为了记录自己生活的点滴，记录自己的心路历程，同时也记录自己思想转变的过程，我相信日后回味起来，这将是一杯世间最醇的“美酒”…… 最后，把我很喜欢的一段话分享给大家，这是国产电影《大鱼·海棠》中女主人公 —— 椿 的一段独白： 你相信奇迹吗？生命是一场旅程，我们等了多少个轮回，才有机会去享受这一次旅程。这短短的一生，我们最终都会失去，不妨大胆一些，爱一个人，攀一座山，追一个梦。是的，不妨大胆一些。很多事我都不了解，很多问题也没有答案。但我相信，上天给我们生命，一定是为了让我们创造奇迹的。—— 电影《大鱼·海棠》]]></content>
      <categories>
        <category>日志</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分分钟教你用node.js写个爬虫]]></title>
    <url>%2F2018%2F06%2F08%2Fnews-spider%2F</url>
    <content type="text"><![CDATA[一、什么是爬虫 网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。WIKIPEDIA 爬虫介绍 二、爬虫的分类 通用网络爬虫（全网爬虫） 爬行对象从一些 种子URL 扩充到整个 Web，主要为门户站点搜索引擎和大型 Web 服务提供商采集数据。 聚焦网络爬虫（主题网络爬虫） 是 指选择性 地爬行那些与预先定义好的主题相关页面的网络爬虫。 增量式网络爬虫 指对已下载网页采取增量式更新和 只爬行新产生的或者已经发生变化网页 的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。 Deep Web 爬虫 爬行对象是一些在用户填入关键字搜索或登录后才能访问到的深层网页信息的爬虫。 三、爬虫的爬行策略 通用网络爬虫（全网爬虫） 深度优先策略、广度优先策略 聚焦网络爬虫（主题网络爬虫） 基于内容评价的爬行策略（内容相关性），基于链接结构评价的爬行策略、基于增强学习的爬行策略（链接重要性），基于语境图的爬行策略（距离，图论中两节点间边的权重） 增量式网络爬虫 统一更新法、个体更新法、基于分类的更新法、自适应调频更新法 Deep Web 爬虫 Deep Web 爬虫爬行过程中最重要部分就是表单填写，包含两种类型：基于领域知识的表单填写、基于网页结构分析的表单填写 现代的网页爬虫的行为通常是四种策略组合的结果： 选择策略：决定所要下载的页面；重新访问策略：决定什么时候检查页面的更新变化；平衡礼貌策略：指出怎样避免站点超载；并行策略：指出怎么协同达到分布式抓取的效果； 四、写一个简单网页爬虫的流程 确定爬取对象（网站/页面） 分析页面内容（目标数据/DOM结构） 确定开发语言、框架、工具等 编码 测试，爬取数据 优化 一个简单的百度新闻爬虫确定爬取对象（网站/页面） 百度新闻 （http://news.baidu.com/） 分析页面内容（目标数据/DOM结构） ······ 确定开发语言、框架、工具等 node.js (express) + SublimeText 3 编码，测试，爬取数据 coding ··· Let’s start新建项目目录 1.在合适的磁盘目录下创建项目目录baiduNews（我的项目目录是：F:\web\baiduNews） 注：因为在写这篇文章的时候用的电脑真心比较渣。安装WebStorm或者VsCode跑项目有些吃力。所以后面的命令行操作我都是在Window自带的DOS命令行窗口中执行的。 初始化package.json 1.在DOS命令行中进入项目根目录 baiduNews2.执行npm init，初始化package.json文件 安装依赖 express （使用express来搭建一个简单的Http服务器。当然，你也可以使用node中自带的http模块）superagent （superagent是node里一个非常方便的、轻量的、渐进式的第三方客户端请求代理模块，用他来请求目标页面）cheerio （cheerio相当于node版的jQuery，用过jQuery的同学会非常容易上手。它主要是用来获取抓取到的页面元素和其中的数据信息） 1234// 个人比较喜欢使用yarn来安装依赖包,当然你也可以使用 npm install 来安装依赖，看个人习惯。yarn add expressyarn add superagentyarn add cheerio 依赖安装完成后你可以在package.json中查看刚才安装的依赖是否成功。安装正确后如下图： 开始coding 一、使用express启动一个简单的本地Http服务器 1、在项目根目录下创建index.js文件（后面都会在这个index文件中进行coding） 2、创建好index.js后，我们首先实例化一个express对象，用它来启动一个本地监听3000端口的Http服务。12345678910const express = require('express');const app = express();// ...let server = app.listen(3000, function () &#123; let host = server.address().address; let port = server.address().port; console.log('Your App is running at http://%s:%s', host, port);&#125;); 对，就是这么简单，不到10行代码，搭建启动一个简单的本地Http服务。 3、按照国际惯例，我们希望在访问本机地址http://localhost:3000的时候，这个服务能给我们犯规一个Hello World！在index.js中加入如下代码：123app.get('/', function (req, res) &#123; res.send('Hello World!');&#125;); 此时，在DOS中项目根目录baiduNews下执行node index.js,让项目跑起来。之后，打开浏览器，访问http://localhost:3000,你就会发现页面上显示’Hellow World!’字样。这样，在后面我们获取到百度新闻首页的信息后，就可以在访问http://localhost:3000时看到这些信息。 二、抓取百度新闻首页的新闻信息 1、 首先，我们先来分析一下百度新闻首页的页面信息。 百度新闻首页大体上分为“热点新闻”、“本地新闻”、“国内新闻”、“国际新闻”……等。这次我们先来尝试抓取左侧“热点新闻”和下方的“本地新闻”两处的新闻数据。 F12打开Chrome的控制台，审查页面元素，经过查看左侧“热点新闻”信息所在DOM的结构，我们发现所有的“热点新闻”信息（包括新闻标题和新闻页面链接）都在id为#pane-news的&lt;div&gt;下面&lt;ul&gt;下&lt;li&gt;下的&lt;a&gt;标签中。用jQuery的选择器表示为：#pane-news ul li a。 2、为了爬取新闻数据，首先我们要用superagent请求目标页面，获取整个新闻首页信息1234567891011121314151617181920// 引入所需要的第三方包const superagent= require('superagent');let hotNews = []; // 热点新闻let localNews = []; // 本地新闻/** * index.js * [description] - 使用superagent.get()方法来访问百度新闻首页 */superagent.get('http://news.baidu.com/').end((err, res) =&gt; &#123; if (err) &#123; // 如果访问失败或者出错，会这行这里 console.log(`热点新闻抓取失败 - $&#123;err&#125;`) &#125; else &#123; // 访问成功，请求http://news.baidu.com/页面所返回的数据会包含在res // 抓取热点新闻数据 hotNews = getHotNews(res) &#125;&#125;); 3、获取页面信息后，我们来定义一个函数getHotNews()来抓取页面内的“热点新闻”数据。12345678910111213141516171819202122232425262728/** * index.js * [description] - 抓取热点新闻页面 */// 引入所需要的第三方包const cheerio = require('cheerio');let getHotNews = (res) =&gt; &#123; let hotNews = []; // 访问成功，请求http://news.baidu.com/页面所返回的数据会包含在res.text中。 /* 使用cheerio模块的cherrio.load()方法，将HTMLdocument作为参数传入函数 以后就可以使用类似jQuery的$(selectior)的方式来获取页面元素 */ let $ = cheerio.load(res.text); // 找到目标数据所在的页面元素，获取数据 $('div#pane-news ul li a').each((idx, ele) =&gt; &#123; // cherrio中$('selector').each()用来遍历所有匹配到的DOM元素 // 参数idx是当前遍历的元素的索引，ele就是当前便利的DOM元素 let news = &#123; title: $(ele).text(), // 获取新闻标题 href: $(ele).attr('href') // 获取新闻网页链接 &#125;; hotNews.push(news) // 存入最终结果数组 &#125;); return hotNews&#125;; 这里要多说几点： async/await据说是异步编程的终级解决方案,它可以让我们以同步的思维方式来进行异步编程。Promise解决了异步编程的“回调地狱”，async/await同时使异步流程控制变得友好而有清晰，有兴趣的同学可以去了解学习一下，真的很好用。 superagent模块提供了很多比如get、post、delte等方法，可以很方便地进行Ajax请求操作。在请求结束后执行.end()回调函数。.end()接受一个函数作为参数，该函数又有两个参数error和res。当请求失败，error会包含返回的错误信息，请求成功，error值为null，返回的数据会包含在res参数中。 cheerio模块的.load()方法，将HTML document作为参数传入函数，以后就可以使用类似jQuery的$(selectior)的方式来获取页面元素。同时可以使用类似于jQuery中的.each()来遍历元素。此外，还有很多方法，大家可以自行Google/Baidu。 4、将抓取的数据返回给前端浏览器 前面，const app = express();实例化了一个express对象app。app.get(&#39;&#39;, async() =&gt; {})接受两个参数，第一个参数接受一个String类型的路由路径，表示Ajax的请求路径。第二个参数接受一个函数Function，当请求此路径时就会执行这个函数中的代码。1234567/** * [description] - 跟路由 */// 当一个get请求 http://localhost:3000时，就会后面的async函数app.get('/', async (req, res, next) =&gt; &#123; res.send(hotNews);&#125;); 在DOS中项目根目录baiduNews下执行node index.js,让项目跑起来。之后，打开浏览器，访问http://localhost:3000,你就会发现抓取到的数据返回到了前端页面。我运行代码后浏览器展示的返回信息如下：注：因为我的Chrome安装了JSONView扩展程序，所以返回的数据在页面展示的时候会被自动格式化为结构性的JSON格式，方便查看。 OK！！这样，一个简单的百度“热点新闻”的爬虫就大功告成啦！！ 简单总结一下，其实步骤很简单： express启动一个简单的Http服务 分析目标页面DOM结构，找到所要抓取的信息的相关DOM元素 使用superagent请求目标页面 使用cheerio获取页面元素，获取目标数据 返回数据到前端浏览器 现在，继续我们的目标，抓取“本地新闻”数据（编码过程中，我们会遇到一些有意思的问题）有了前面的基础，我们自然而然的会想到利用和上面相同的方法“本地新闻”数据。1、 分析页面中“本地新闻”部分的DOM结构，如下图： F12打开控制台，审查“本地新闻”DOM元素，我们发现，“本地新闻”分为两个主要部分，“左侧新闻”和右侧的“新闻资讯”。这所有目标数据都在id为#local_news的div中。“左侧新闻”数据又在id为#localnews-focus的ul标签下的li标签下的a标签中，包括新闻标题和页面链接。“本地资讯”数据又在id为#localnews-zixun的div下的ul标签下的li标签下的a标签中，包括新闻标题和页面链接。 2、OK！分析了DOM结构，确定了数据的位置，接下来和爬取“热点新闻”一样，按部就班，定义一个getLocalNews()函数，爬取这些数据。 123456789101112131415161718192021222324252627/** * [description] - 抓取本地新闻页面 */let getLocalNews = (res) =&gt; &#123; let localNews = []; let $ = cheerio.load(res); // 本地新闻 $('ul#localnews-focus li a').each((idx, ele) =&gt; &#123; let news = &#123; title: $(ele).text(), href: $(ele).attr('href'), &#125;; localNews.push(news) &#125;); // 本地资讯 $('div#localnews-zixun ul li a').each((index, item) =&gt; &#123; let news = &#123; title: $(item).text(), href: $(item).attr('href') &#125;; localNews.push(news); &#125;); return localNews&#125;; 对应的，在superagent.get()中请求页面后，我们需要调用getLocalNews()函数，来爬去本地新闻数据。superagent.get()函数修改为：1234567891011superagent.get('http://news.baidu.com/').end((err, res) =&gt; &#123; if (err) &#123; // 如果访问失败或者出错，会这行这里 console.log(`热点新闻抓取失败 - $&#123;err&#125;`) &#125; else &#123; // 访问成功，请求http://news.baidu.com/页面所返回的数据会包含在res // 抓取热点新闻数据 hotNews = getHotNews(res) localNews = getLocalNews(res) &#125;&#125;); 同时，我们要在app.get()路由中也要将数据返回给前端浏览器。app.get()路由代码修改为：12345678910/** * [description] - 跟路由 */// 当一个get请求 http://localhost:3000时，就会后面的async函数app.get('/', async (req, res, next) =&gt; &#123; res.send(&#123; hotNews: hotNews, localNews: localNews &#125;);&#125;); 编码完成，激动不已！！DOS中让项目跑起来，用浏览器访问http://localhost:3000 尴尬的事情发生了！！返回的数据只有热点新闻，而本地新闻返回一个空数组[ ]。检查代码，发现也没有问题，但为什么一直返回的空数组呢？经过一番原因查找，才返现问题出在哪里！！ 一个有意思的问题 为了找到原因，首先，我们看看用superagent.get(&#39;http://news.baidu.com/&#39;).end((err, res) =&gt; {})请求百度新闻首页在回调函数.end()中的第二个参数res中到底拿到了什么内容？12345678910111213141516171819202122232425// 新定义一个全局变量 pageReslet pageRes = &#123;&#125;; // supergaent页面返回值// superagent.get()中将res存入pageRessuperagent.get('http://news.baidu.com/').end((err, res) =&gt; &#123; if (err) &#123; // 如果访问失败或者出错，会这行这里 console.log(`热点新闻抓取失败 - $&#123;err&#125;`) &#125; else &#123; // 访问成功，请求http://news.baidu.com/页面所返回的数据会包含在res // 抓取热点新闻数据 // hotNews = getHotNews(res) // localNews = getLocalNews(res) pageRes = res &#125;&#125;);// 将pageRes返回给前端浏览器，便于查看app.get('/', async (req, res, next) =&gt; &#123; res.send(&#123; // &#123;&#125;hotNews: hotNews, // localNews: localNews, pageRes: pageRes &#125;);&#125;); 访问浏览器http://localhost:3000，页面展示如下内容： 可以看到，返回值中的text字段应该就是整个页面的HTML代码的字符串格式。为了方便我们观察，可以直接把这个text字段值返回给前端浏览器，这样我们就能够清晰地看到经过浏览器渲染后的页面。 修改给前端浏览器的返回值123app.get('/', async (req, res, next) =&gt; &#123; res.send(pageRes.text)&#125; 访问浏览器http://localhost:3000，页面展示如下内容： 审查元素才发现，原来我们抓取的目标数据所在的DOM元素中是空的，里面没有数据！到这里，一切水落石出！在我们使用superagent.get()访问百度新闻首页时，res中包含的获取的页面内容中，我们想要的“本地新闻”数据还没有生成，DOM节点元素是空的，所以出现前面的情况！抓取后返回的数据一直是空数组[ ]。 在控制台的Network中我们发现页面请求了一次这样的接口：http://localhost:3000/widget?id=LocalNews&amp;ajax=json&amp;t=1526295667917，接口状态 404。这应该就是百度新闻获取“本地新闻”的接口，到这里一切都明白了！“本地新闻”是在页面加载后动态请求上面这个接口获取的，所以我们用superagent.get()请求的页面再去请求这个接口时，接口URL中hostname部分变成了本地IP地址，而本机上没有这个接口，所以404，请求不到数据。 找到原因，我们来想办法解决这个问题！！ 直接使用superagent访问正确合法的百度“本地新闻”的接口，获取数据后返回给前端浏览器。 使用第三方npm包，模拟浏览器访问百度新闻首页，在这个模拟浏览器中当“本地新闻”加载成功后，抓取数据，返回给前端浏览器。 以上方法均可，我们来试试比较有意思的第二种方法 使用Nightmare自动化测试工具 Electron可以让你使用纯JavaScript调用Chrome丰富的原生的接口来创造桌面应用。你可以把它看作一个专注于桌面应用的Node.js的变体，而不是Web服务器。其基于浏览器的应用方式可以极方便的做各种响应式的交互 Nightmare是一个基于Electron的框架，针对Web自动化测试和爬虫，因为其具有跟PlantomJS一样的自动化测试的功能可以在页面上模拟用户的行为触发一些异步数据加载，也可以跟Request库一样直接访问URL来抓取数据，并且可以设置页面的延迟时间，所以无论是手动触发脚本还是行为触发脚本都是轻而易举的。 安装依赖12// 安装nightmareyarn add nightmare 为获取“本地新闻”，继续coding…给index.js中新增如下代码：123456789101112131415161718192021const Nightmare = require('nightmare'); // 自动化测试包，处理动态页面const nightmare = Nightmare(&#123; show: true &#125;); // show:true 显示内置模拟浏览器/** * [description] - 抓取本地新闻页面 * [nremark] - 百度本地新闻在访问页面后加载js定位IP位置后获取对应新闻， * 所以抓取本地新闻需要使用 nightmare 一类的自动化测试工具， * 模拟浏览器环境访问页面，使js运行，生成动态页面再抓取 */// 抓取本地新闻页面nightmare.goto('http://news.baidu.com/').wait("div#local_news").evaluate(() =&gt; document.querySelector("div#local_news").innerHTML).then(htmlStr =&gt; &#123; // 获取本地新闻数据 localNews = getLocalNews(htmlStr)&#125;).catch(error =&gt; &#123; console.log(`本地新闻抓取失败 - $&#123;error&#125;`);&#125;) 修改getLocalNews()函数为：123456789101112131415161718192021222324252627/** * [description]- 获取本地新闻数据 */let getLocalNews = (htmlStr) =&gt; &#123; let localNews = []; let $ = cheerio.load(htmlStr); // 本地新闻 $('ul#localnews-focus li a').each((idx, ele) =&gt; &#123; let news = &#123; title: $(ele).text(), href: $(ele).attr('href'), &#125;; localNews.push(news) &#125;); // 本地资讯 $('div#localnews-zixun ul li a').each((index, item) =&gt; &#123; let news = &#123; title: $(item).text(), href: $(item).attr('href') &#125;; localNews.push(news); &#125;); return localNews&#125; 修改app.get(&#39;/&#39;)路由为：12345678910/** * [description] - 跟路由 */// 当一个get请求 http://localhost:3000时，就会后面的async函数app.get('/', async (req, res, next) =&gt; &#123; res.send(&#123; hotNews: hotNews, localNews: localNews &#125;)&#125;); 此时，DOS命令行中重新让项目跑起来，浏览器访问https://localhost:3000，看看页面展示的信息，看是否抓取到了“本地新闻”数据！ 至此，一个简单而又完整的抓取百度新闻页面“热点新闻”和“本地新闻”的爬虫就大功告成啦！！ 最后总结一下，整体思路如下： express启动一个简单的Http服务 分析目标页面DOM结构，找到所要抓取的信息的相关DOM元素 使用superagent请求目标页面 动态页面（需要加载页面后运行JS或请求接口的页面）可以使用Nightmare模拟浏览器访问 使用cheerio获取页面元素，获取目标数据 ###完整代码 爬虫完整代码GitHub地址：完整代码 后面，应该还会做一些进阶，来爬取某些网站上比较好看的图片（手动滑稽），会牵扯到并发控制和反-反爬虫的一些策略。再用爬虫取爬去一些需要登录和输入验证码的网站，欢迎到时大家关注和指正交流。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>Node.JS</tag>
      </tags>
  </entry>
</search>
